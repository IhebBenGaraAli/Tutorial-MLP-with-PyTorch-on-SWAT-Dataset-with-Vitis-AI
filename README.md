# Tutoriel : MLP avec PyTorch sur le Dataset SWAT et Vitis AI

## ğŸ“Œ Introduction

Ce tutoriel vous guide Ã  travers les Ã©tapes nÃ©cessaires pour entraÃ®ner, quantifier et dÃ©ployer un modÃ¨le **MLP (Multi-Layer Perceptron)** avec **PyTorch** sur le dataset **SWAT**, en utilisant **Vitis AI** pour l'optimisation et l'exÃ©cution sur un **DPU (Deep Learning Processing Unit)**. Vous apprendrez Ã  :

1. EntraÃ®ner un modÃ¨le MLP avec PyTorch. 
2. Installer et configurer Vitis AI.
3. Quantifier le modÃ¨le pour une exÃ©cution optimisÃ©e sur FPGA.
4. DÃ©ployer et exÃ©cuter le modÃ¨le sur une carte cible.
5. Mesurer la consommation Ã©nergÃ©tique pendant l'infÃ©rence.

---

## ğŸ“¥ Installation de Vitis AI

### Clonage du dÃ©pÃ´t Vitis AI

Commencez par cloner le dÃ©pÃ´t officiel de Vitis AI :

```sh
git clone https://github.com/Xilinx/Vitis-AI
```

### Configuration de l'environnement

Dans ce tutoriel, nous supposons que vous avez installÃ© **Vitis AI 3.5** (abrÃ©gÃ© VAI3.5) dans votre systÃ¨me de fichiers. DÃ©finissez le rÃ©pertoire de travail comme suit :

```sh
export WRK_DIR=/media/danieleb/DATA/VAI3.5
```

### Construction de l'image Docker

Depuis le rÃ©pertoire **Vitis AI 3.5**, exÃ©cutez les commandes suivantes pour construire l'image Docker :

```sh
cd ${WRK_DIR}
cd docker
./docker_build.sh -t gpu -f pytorch
```

Une fois terminÃ©, vÃ©rifiez l'image Docker crÃ©Ã©e :

```sh
docker images
```

Vous devriez voir une sortie similaire Ã  :

```
REPOSITORY                        TAG               IMAGE ID       CREATED         SIZE
xilinx/vitis-ai-pytorch-gpu   3.5.0.001-b56bcce50   3c5d174a1807   27 hours ago    21.4GB
```

---

## ğŸ“‚ RÃ©pertoire de travail

### CrÃ©ation du rÃ©pertoire du tutoriel

CrÃ©ez un dossier **tutorials** sous **${WRK_DIR}**, puis copiez ce tutoriel dans ce dossier et renommez-le en **Tutorial-MLP**.

Avec la commande suivante, vous pouvez vÃ©rifier la structure des dossiers :

```sh
tree -d -L 2
```

Vous devriez voir une structure similaire Ã  :

```
${WRK_DIR} # votre rÃ©pertoire de travail Vitis AI 3.5
.
â”œâ”€â”€ bck
â”œâ”€â”€ board_setup
â”‚   â”œâ”€â”€ v70
â”‚   â””â”€â”€ vek280
â”œâ”€â”€ demos
â”œâ”€â”€ docker
â”‚   â”œâ”€â”€ common
â”‚   â”œâ”€â”€ conda
â”‚   â””â”€â”€ dockerfiles
â”œâ”€â”€ docs
â”‚   â”œâ”€â”€ docs
â”‚   â”œâ”€â”€ _downloads
â”‚   â”œâ”€â”€ doxygen
â”‚   â”œâ”€â”€ _images
â”‚   â”œâ”€â”€ _sources
â”‚   â””â”€â”€ _static
â”œâ”€â”€ docsrc
â”‚   â”œâ”€â”€ build
â”‚   â””â”€â”€ source
â”œâ”€â”€ dpu
â”œâ”€â”€ examples
â”‚   â”œâ”€â”€ custom_operator
â”‚   â”œâ”€â”€ ofa
â”‚   â”œâ”€â”€ OnBoard
â”‚   â”œâ”€â”€ vai_library
â”‚   â”œâ”€â”€ vai_optimizer
â”‚   â”œâ”€â”€ vai_profiler
â”‚   â”œâ”€â”€ vai_quantizer
â”‚   â”œâ”€â”€ vai_runtime
â”‚   â”œâ”€â”€ waa
â”‚   â””â”€â”€ wego
â”œâ”€â”€ model_zoo
â”‚   â”œâ”€â”€ images
â”‚   â””â”€â”€ model-list
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ AKS
â”‚   â”œâ”€â”€ vai_library
â”‚   â”œâ”€â”€ vai_optimizer
â”‚   â”œâ”€â”€ vai_petalinux_recipes
â”‚   â”œâ”€â”€ vai_quantizer
â”‚   â””â”€â”€ vai_runtime
â”œâ”€â”€ third_party
â”‚   â”œâ”€â”€ tflite
â”‚   â””â”€â”€ tvm
â””â”€â”€ tutorials # crÃ©Ã© par vous
    â”œâ”€â”€ Tutorial-MLP # ce tutoriel
```

---

## ğŸš€ Lancement du conteneur Docker

### DÃ©marrage du conteneur

Depuis **${WRK_DIR}**, lancez le conteneur Docker Vitis AI :

```sh
cd ${WRK_DIR}
./docker_run.sh xilinx/vitis-ai-pytorch-gpu:latest
```

### Activation de l'environnement

Activez l'environnement Conda et accÃ©dez au rÃ©pertoire du tutoriel :

```sh
conda activate vitis-ai-pytorch
cd /workspace/tutorials/Tutorial-MLP
```

ğŸ’¡ **Remarque** : Le dossier `/workspace` dans le conteneur est mappÃ© au systÃ¨me de fichiers hÃ´te.

---

## ğŸ“Š Dataset SWAT

### TÃ©lÃ©chargement du dataset

Nous utilisons le dataset **SWaT_Dataset_Attack_v0.csv**, disponible ici :

ğŸ”— [TÃ©lÃ©charger le dataset](https://itrust.sutd.edu.sg/itrust-labs_datasets/dataset_info/)

### CaractÃ©ristiques du dataset

- **11 jours d'opÃ©rations** (7 jours normaux, 4 jours avec attaques).
- **51 capteurs et actionneurs** surveillÃ©s.
- **41 attaques simulÃ©es**.

### Placement du dataset  

Dans le rÃ©pertoire `train_mlp`, placez le dataset nÃ©cessaire Ã  l'entraÃ®nement du modÃ¨le.  

Dans `vitis_ai_mlp`, crÃ©ez un dossier `build`, puis Ã  l'intÃ©rieur de `build`, ajoutez deux sous-rÃ©pertoires : `data` et `float`.  

Le dataset doit Ã©galement Ãªtre placÃ© dans le repertoire swat_dataset :  

```sh
/workspace/tutorials/Tutorial-MLP/vitis_ai_mlp/build/data/swat_dataset
```

---

## ğŸ¯ EntraÃ®nement du modÃ¨le MLP

### ExÃ©cution du script d'entraÃ®nement

Dans le rÃ©pertoire **train_mlp**, exÃ©cutez le script **train_mlp_pytorch.ipynb** pour entraÃ®ner un MLP avec PyTorch.

- Le modÃ¨le atteint une **prÃ©cision de 98%**.
- Les poids sont sauvegardÃ©s en **format `.pt`.
- copier le fichier `.pt` ** dans **`/workspace/tutorials/Tutorial-MLP/vitis_ai_mlp/build/float`**.

---

## ğŸ” Quantification du modÃ¨le

### ExÃ©cution de la quantification

ExÃ©cutez le script de quantification :

```sh
cd /workspace/vitis_ai_mlp
./scripts/quant.sh
```
Une fois la quantification terminÃ©e, vous pouvez vÃ©rifier les rÃ©sultats avec Netron. Vous devriez y voir "fix 256", ce qui fait rÃ©fÃ©rence Ã  l'utilisation du format INT8. Cela signifie que Vitis AI a quantifiÃ© les poids, les rÃ©duisant de leur format flottant Ã  une reprÃ©sentation sur 8 bits (INT8).

---

## ğŸ›  Compilation pour le DPU

### Adaptation du fichier `arch.json`

Adaptez le fichier **arch.json** pour correspondre au fingerprint du DPU cible.

### Compilation du modÃ¨le

ExÃ©cutez la compilation :

```sh
cd /workspace/vitis_ai_mlp
./comp.sh
```
Si tout se passe bien, la compilation gÃ©nÃ¨re le fichier **_MLP_int.xmodel.xmodel** dans le rÃ©pertoire **build/compiled_**.

---

## ğŸƒâ€â™‚ï¸ ExÃ©cution sur la carte cible

### DÃ©ploiement du modÃ¨le

AprÃ¨s la compilation, copiez le modÃ¨le **`.xmodel`** et le dataset dans **`inference_dpu_mlp`**, puis transfÃ©rez-les sur la carte cible.

---

## âš¡ Mesure de la consommation

### Mesure de la consommation Ã©nergÃ©tique

Dans **`inference_dpu_mlp`**, utilisez le script **power_measurement2** pour mesurer la consommation Ã©nergÃ©tique durant l'infÃ©rence.

---

## ğŸ Conclusion

Ce tutoriel vous a accompagnÃ© tout au long du processus d'entraÃ®nement, de quantification et d'exÃ©cution d'un modÃ¨le MLP avec **Vitis AI** sur le dataset **SWAT**. Vous avez aussi appris Ã  mesurer la consommation Ã©nergÃ©tique pendant l'infÃ©rence.  
Attention : il vous revient de gÃ©rer les erreurs de versions de librairies qui peuvent parfois apparaÃ®tre.

---
